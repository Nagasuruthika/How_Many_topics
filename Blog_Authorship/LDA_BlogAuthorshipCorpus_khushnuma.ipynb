{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BlogAuthorshipCorpus.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_ShVfmolnA6"
      },
      "source": [
        "## **Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy7RnwvuLAnC",
        "outputId": "f65358e2-a7af-4d23-f7cb-61536e689778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# mounting my google drive \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dACoh75-MH4a"
      },
      "source": [
        "!cp 'drive/My Drive/Datasets/archive.zip' '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjOlFBBCLdPA",
        "outputId": "a734b909-f8b4-4c1d-96b3-415b0de74cae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# unzipping the dataset\n",
        "\n",
        "!unzip archive.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  archive.zip\n",
            "  inflating: blogtext.csv            \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLaY7-fBMt84"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"blogtext.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slundJ6RM8ML",
        "outputId": "0e98cbdc-905b-4a5d-8583-4b79000eca81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>topic</th>\n",
              "      <th>sign</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>14,May,2004</td>\n",
              "      <td>Info has been found (+/- 100 pages,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>13,May,2004</td>\n",
              "      <td>These are the team members:   Drewe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>In het kader van kernfusie op aarde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2059027</td>\n",
              "      <td>male</td>\n",
              "      <td>15</td>\n",
              "      <td>Student</td>\n",
              "      <td>Leo</td>\n",
              "      <td>12,May,2004</td>\n",
              "      <td>testing!!!  testing!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3581210</td>\n",
              "      <td>male</td>\n",
              "      <td>33</td>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Aquarius</td>\n",
              "      <td>11,June,2004</td>\n",
              "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ...                                               text\n",
              "0  2059027  ...             Info has been found (+/- 100 pages,...\n",
              "1  2059027  ...             These are the team members:   Drewe...\n",
              "2  2059027  ...             In het kader van kernfusie op aarde...\n",
              "3  2059027  ...                   testing!!!  testing!!!          \n",
              "4  3581210  ...               Thanks to Yahoo!'s Toolbar I can ...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jSMWXE2lwEI"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzwQ2PI4l8C1"
      },
      "source": [
        "### **removing unnecessary columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "333XF3GxM-jG"
      },
      "source": [
        "train = train[['topic','text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Nu8_JCYNZAZ",
        "outputId": "c61e2c59-c37f-463e-9142-375dc3177bb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Student</td>\n",
              "      <td>Info has been found (+/- 100 pages,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Student</td>\n",
              "      <td>These are the team members:   Drewe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Student</td>\n",
              "      <td>In het kader van kernfusie op aarde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Student</td>\n",
              "      <td>testing!!!  testing!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               topic                                               text\n",
              "0            Student             Info has been found (+/- 100 pages,...\n",
              "1            Student             These are the team members:   Drewe...\n",
              "2            Student             In het kader van kernfusie op aarde...\n",
              "3            Student                   testing!!!  testing!!!          \n",
              "4  InvestmentBanking               Thanks to Yahoo!'s Toolbar I can ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r72QmIg_mF9d"
      },
      "source": [
        "### **steps**:\n",
        "* Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
        "* Words that have fewer than 3 characters are removed.\n",
        "* All stopwords are removed.\n",
        "* Words are lemmatized — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
        "* Words are stemmed — words are reduced to their root form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMjk-SVrl4NQ",
        "outputId": "87c3fb35-be19-4215-f57a-b38272aca784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(13)\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5wyZWCd14mF"
      },
      "source": [
        "stemmer = SnowballStemmer('english')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pLgcIFO1Wvz"
      },
      "source": [
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nFtJ-151Yb3",
        "outputId": "55972230-0faa-40e2-9283-e0e2d4fdd2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "doc_sample = train['text'][0]\n",
        "print('original document: ')\n",
        "words = []\n",
        "for word in doc_sample.split(' '):\n",
        "    words.append(word)\n",
        "print(words)\n",
        "print('\\n\\n tokenized and lemmatized document: ')\n",
        "print(preprocess(doc_sample))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original document: \n",
            "['', '', '', '', '', '', '', '', '', '', '', 'Info', 'has', 'been', 'found', '(+/-', '100', 'pages,', 'and', '4.5', 'MB', 'of', '.pdf', 'files)', 'Now', 'i', 'have', 'to', 'wait', 'untill', 'our', 'team', 'leader', 'has', 'processed', 'it', 'and', 'learns', 'html.', '', '', '', '', '', '', '', '', '']\n",
            "\n",
            "\n",
            " tokenized and lemmatized document: \n",
            "['info', 'page', 'file', 'wait', 'until', 'team', 'leader', 'process', 'learn', 'html']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgGmXJGX1K0s"
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvonwjNu2Kqc"
      },
      "source": [
        "train['clean_doc'] = train['text'].progress_map(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goVHeadjwFQ8",
        "outputId": "9b1555af-da26-4edf-c699-9fb571ee2d44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>text</th>\n",
              "      <th>clean_doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Student</td>\n",
              "      <td>Info has been found (+/- 100 pages,...</td>\n",
              "      <td>[info, page, file, wait, until, team, leader, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Student</td>\n",
              "      <td>These are the team members:   Drewe...</td>\n",
              "      <td>[team, member, drew, laag, urllink, mail, ruiy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Student</td>\n",
              "      <td>In het kader van kernfusie op aarde...</td>\n",
              "      <td>[kader, kernfusi, aard, maak, eigen, waterstof...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Student</td>\n",
              "      <td>testing!!!  testing!!!</td>\n",
              "      <td>[test, test]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
              "      <td>[thank, yahoo, toolbar, captur, url, popup, me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>I had an interesting conversation...</td>\n",
              "      <td>[interest, convers, morn, talk, korean, money,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Somehow Coca-Cola has a way of su...</td>\n",
              "      <td>[coca, cola, sum, thing, earli, flagship, jing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>If anything, Korea is a country o...</td>\n",
              "      <td>[korea, countri, extrem, base, think, come, ko...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>Take a read of this news article ...</td>\n",
              "      <td>[read, news, articl, urllink, joongang, ilbo, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>I surf the English news sites a l...</td>\n",
              "      <td>[surf, english, news, sit, look, tidbit, korea...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               topic  ...                                          clean_doc\n",
              "0            Student  ...  [info, page, file, wait, until, team, leader, ...\n",
              "1            Student  ...  [team, member, drew, laag, urllink, mail, ruiy...\n",
              "2            Student  ...  [kader, kernfusi, aard, maak, eigen, waterstof...\n",
              "3            Student  ...                                       [test, test]\n",
              "4  InvestmentBanking  ...  [thank, yahoo, toolbar, captur, url, popup, me...\n",
              "5  InvestmentBanking  ...  [interest, convers, morn, talk, korean, money,...\n",
              "6  InvestmentBanking  ...  [coca, cola, sum, thing, earli, flagship, jing...\n",
              "7  InvestmentBanking  ...  [korea, countri, extrem, base, think, come, ko...\n",
              "8  InvestmentBanking  ...  [read, news, articl, urllink, joongang, ilbo, ...\n",
              "9  InvestmentBanking  ...  [surf, english, news, sit, look, tidbit, korea...\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBksRUbU0gmQ"
      },
      "source": [
        "train = train.drop(['text'], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8dXs4cd0vuV",
        "outputId": "ae203f95-822d-436d-e759-433311589d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic</th>\n",
              "      <th>clean_doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Student</td>\n",
              "      <td>[info, page, file, wait, until, team, leader, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Student</td>\n",
              "      <td>[team, member, drew, laag, urllink, mail, ruiy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Student</td>\n",
              "      <td>[kader, kernfusi, aard, maak, eigen, waterstof...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Student</td>\n",
              "      <td>[test, test]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>[thank, yahoo, toolbar, captur, url, popup, me...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>[interest, convers, morn, talk, korean, money,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>[coca, cola, sum, thing, earli, flagship, jing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>[korea, countri, extrem, base, think, come, ko...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>[read, news, articl, urllink, joongang, ilbo, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>InvestmentBanking</td>\n",
              "      <td>[surf, english, news, sit, look, tidbit, korea...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               topic                                          clean_doc\n",
              "0            Student  [info, page, file, wait, until, team, leader, ...\n",
              "1            Student  [team, member, drew, laag, urllink, mail, ruiy...\n",
              "2            Student  [kader, kernfusi, aard, maak, eigen, waterstof...\n",
              "3            Student                                       [test, test]\n",
              "4  InvestmentBanking  [thank, yahoo, toolbar, captur, url, popup, me...\n",
              "5  InvestmentBanking  [interest, convers, morn, talk, korean, money,...\n",
              "6  InvestmentBanking  [coca, cola, sum, thing, earli, flagship, jing...\n",
              "7  InvestmentBanking  [korea, countri, extrem, base, think, come, ko...\n",
              "8  InvestmentBanking  [read, news, articl, urllink, joongang, ilbo, ...\n",
              "9  InvestmentBanking  [surf, english, news, sit, look, tidbit, korea..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3tcGQ2JwIgE"
      },
      "source": [
        "train.to_csv('/content/drive/My Drive/preprocessed_blog.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKWGBXsMrJd7"
      },
      "source": [
        "### **Exploratory Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qJqSjjdwsPG"
      },
      "source": [
        "# slow method\n",
        "\n",
        "# train = pd.read_csv(\"/content/drive/My Drive/preprocessed_blog.csv\")\n",
        "# print(train.head(10))\n",
        "\n",
        "# from ast import literal_eval\n",
        "# train['clean_doc'] = train['clean_doc'].map(literal_eval)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2IRwNSb6DH8"
      },
      "source": [
        "train.to_pickle('/content/drive/My Drive/preprocessed_blog.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP9_dskt6YJt"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_pickle('/content/drive/My Drive/preprocessed_blog.pkl')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4Jdlrgd5DeE",
        "outputId": "446e02c0-43a4-4453-e769-f1928145ad06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train['clean_doc'][0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['info', 'page', 'file', 'wait', 'until', 'team', 'leader', 'process', 'learn', 'html']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61-CPELWN2sE",
        "outputId": "5d66c1f5-c302-44b1-d423-4d7a9be28516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train.count()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0    681284\n",
              "topic         681284\n",
              "clean_doc     681284\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-yFt82qOHm6",
        "outputId": "f8e890b4-3e4c-4f02-c3f5-26e4380a6c1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "train['topic'].value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "indUnk                     251015\n",
              "Student                    153903\n",
              "Technology                  42055\n",
              "Arts                        32449\n",
              "Education                   29633\n",
              "Communications-Media        20140\n",
              "Internet                    16006\n",
              "Non-Profit                  14700\n",
              "Engineering                 11653\n",
              "Law                          9040\n",
              "Publishing                   7753\n",
              "Science                      7269\n",
              "Government                   6907\n",
              "Consulting                   5862\n",
              "Religion                     5235\n",
              "Fashion                      4851\n",
              "Marketing                    4769\n",
              "Advertising                  4676\n",
              "BusinessServices             4500\n",
              "Banking                      4049\n",
              "Chemicals                    3928\n",
              "Telecommunications           3891\n",
              "Accounting                   3832\n",
              "Military                     3128\n",
              "Museums-Libraries            3096\n",
              "Sports-Recreation            3038\n",
              "HumanResources               3010\n",
              "RealEstate                   2870\n",
              "Transportation               2326\n",
              "Manufacturing                2272\n",
              "Biotech                      2234\n",
              "Tourism                      1942\n",
              "LawEnforcement-Security      1878\n",
              "Architecture                 1638\n",
              "InvestmentBanking            1292\n",
              "Automotive                   1244\n",
              "Agriculture                  1235\n",
              "Construction                 1093\n",
              "Environment                   592\n",
              "Maritime                      280\n",
              "Name: topic, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSLr1dxV1rhy",
        "outputId": "24827bff-7c81-4379-f4fa-e36c723b8346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train['topic'].value_counts())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvyr0-cEAg7e"
      },
      "source": [
        "## **Bag of Words on the clean docs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx-0J-QECnJB"
      },
      "source": [
        "### **dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYvpb7XmwfTA"
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(train['clean_doc'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PPy95qmAwGi",
        "outputId": "18a4fa5f-61e9-4596-bb8b-a459155f9ff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "for i in range(20):\n",
        "    print(i, dictionary[i])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 file\n",
            "1 html\n",
            "2 info\n",
            "3 leader\n",
            "4 learn\n",
            "5 page\n",
            "6 process\n",
            "7 team\n",
            "8 until\n",
            "9 wait\n",
            "10 bryan\n",
            "11 drew\n",
            "12 mail\n",
            "13 member\n",
            "14 urllink\n",
            "15 abl\n",
            "16 absolut\n",
            "17 accident\n",
            "18 accomplish\n",
            "19 accord\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgcv6wdcCCNo"
      },
      "source": [
        "### **Filter out tokens that appear in**\n",
        "* less than 15 documents (absolute number) or\n",
        "* more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
        "* after the above two steps, keep only the first 100000 most frequent tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9wesOyRBYav"
      },
      "source": [
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icpEa7vwCM9q",
        "outputId": "5985171d-f87c-4f1e-e437-5a5f96aa4a99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "for i in range(20):\n",
        "    print(i, dictionary[i])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 file\n",
            "1 html\n",
            "2 info\n",
            "3 leader\n",
            "4 learn\n",
            "5 page\n",
            "6 process\n",
            "7 team\n",
            "8 until\n",
            "9 wait\n",
            "10 bryan\n",
            "11 drew\n",
            "12 mail\n",
            "13 member\n",
            "14 urllink\n",
            "15 abl\n",
            "16 absolut\n",
            "17 accident\n",
            "18 accomplish\n",
            "19 accord\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzDNz_iT5YK7"
      },
      "source": [
        "with open('/content/drive/My Drive/dictionary.pickle', 'wb') as handle:\n",
        "    pickle.dump(dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH4K4VGo5bR2"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/dictionary.pickle', 'rb') as handle:\n",
        "    dictionary = pickle.load(handle)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baTcY5oNCi6i"
      },
      "source": [
        "### **Gensim doc2bow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jptc-lmCV3G"
      },
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in train['clean_doc']]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGS1qYpc4b98"
      },
      "source": [
        "with open('/content/drive/My Drive/bow_corpus.pickle', 'wb') as handle:\n",
        "    pickle.dump(bow_corpus, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHYoA63l4kwn"
      },
      "source": [
        "with open('/content/drive/My Drive/bow_corpus.pickle', 'rb') as handle:\n",
        "    bow_corpus = pickle.load(handle)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOJPHrt5EBil"
      },
      "source": [
        "from pprint import pprint"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUwsMzbkC-C9",
        "outputId": "e31f7341-e18c-4e44-bb50-84ae7274a02f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('Preprocessed Text')\n",
        "print(train['clean_doc'][500])\n",
        "for i in range(len(bow_corpus[500])):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_corpus[500][i][0], \n",
        "                                                    dictionary[bow_corpus[500][i][0]], \n",
        "                                                    bow_corpus[500][i][1]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessed Text\n",
            "['monday', 'start', 'possibl', 'process', 'buy', 'place', 'realtor', 'paper', 'sign', 'go', 'commit', 'scari', 'thing', 'idea', 'home', 'ownership', 'sign', 'away', 'life', 'year', 'good', 'idea', 'financi', 'mental', 'good', 'singl', 'girlfriend', 'buy', 'place', 'friend', 'richmond', 'buy', 'hous', 'think', 'ball', 'home', 'ownership', 'deal', 'hous', 'deal', 'person', 'mow', 'prune', 'roof', 'replac', 'window', 'wash', 'bare', 'handl', 'keep', 'room', 'apart', 'order', 'shape', 'imagin', 'letter', 'neighbor', 'concern', 'jungl', 'certain', 'grow', 'outsid', 'home', 'condo', 'condo', 'sound', 'good', 'like', 'apart', 'paint', 'wall', 'deduct', 'condo', 'search', 'hard', 'begin', 'stag', 'see', 'person', 'bunch', 'onlin', 'catch', 'think', 'wait', 'perfect', 'fall', 'picki', 'open', 'mind', 'tell', 'realtor', 'interest', 'citi', 'properti', 'construct', 'want', 'live', 'wear', 'wise', 'place', 'nice', 'tri', 'sell', 'place', 'realtor', 'take', 'note', 'check', 'ring', 'attract', 'marri', 'mean', 'love', 'win', 'person', 'bright', 'smile', 'blog', 'want', 'relationship', 'blog', 'want', 'littl', 'guess', 'friend', 'expect', 'good', 'friend', 'get', 'marri', 'summer', 'tell', 'singl', 'girl', 'claim', 'singl', 'watch', 'felt', 'year', 'poof', 'get', 'marri', 'relationship', 'phobia', 'haven', 'expect', 'year', 'unexpect', 'readi', 'commit', 'readi', 'commit', 'condo', 'sure']\n",
            "Word 6 (\"process\") appears 1 time.\n",
            "Word 9 (\"wait\") appears 1 time.\n",
            "Word 44 (\"apart\") appears 2 time.\n",
            "Word 58 (\"attract\") appears 1 time.\n",
            "Word 70 (\"begin\") appears 1 time.\n",
            "Word 131 (\"certain\") appears 1 time.\n",
            "Word 137 (\"check\") appears 1 time.\n",
            "Word 146 (\"citi\") appears 1 time.\n",
            "Word 171 (\"construct\") appears 1 time.\n",
            "Word 289 (\"expect\") appears 2 time.\n",
            "Word 343 (\"friend\") appears 3 time.\n",
            "Word 360 (\"get\") appears 2 time.\n",
            "Word 361 (\"girl\") appears 1 time.\n",
            "Word 366 (\"go\") appears 1 time.\n",
            "Word 369 (\"good\") appears 4 time.\n",
            "Word 384 (\"handl\") appears 1 time.\n",
            "Word 386 (\"hard\") appears 1 time.\n",
            "Word 387 (\"haven\") appears 1 time.\n",
            "Word 404 (\"home\") appears 3 time.\n",
            "Word 413 (\"hous\") appears 2 time.\n",
            "Word 426 (\"idea\") appears 2 time.\n",
            "Word 455 (\"interest\") appears 1 time.\n",
            "Word 494 (\"life\") appears 1 time.\n",
            "Word 496 (\"like\") appears 1 time.\n",
            "Word 500 (\"littl\") appears 1 time.\n",
            "Word 501 (\"live\") appears 1 time.\n",
            "Word 526 (\"mean\") appears 1 time.\n",
            "Word 565 (\"neighbor\") appears 1 time.\n",
            "Word 574 (\"note\") appears 1 time.\n",
            "Word 587 (\"open\") appears 1 time.\n",
            "Word 589 (\"order\") appears 1 time.\n",
            "Word 597 (\"ownership\") appears 2 time.\n",
            "Word 621 (\"place\") appears 4 time.\n",
            "Word 637 (\"possibl\") appears 1 time.\n",
            "Word 658 (\"properti\") appears 1 time.\n",
            "Word 691 (\"readi\") appears 2 time.\n",
            "Word 721 (\"room\") appears 1 time.\n",
            "Word 745 (\"see\") appears 1 time.\n",
            "Word 749 (\"sell\") appears 1 time.\n",
            "Word 757 (\"shape\") appears 1 time.\n",
            "Word 792 (\"sound\") appears 1 time.\n",
            "Word 806 (\"start\") appears 1 time.\n",
            "Word 842 (\"sure\") appears 1 time.\n",
            "Word 852 (\"take\") appears 1 time.\n",
            "Word 861 (\"tell\") appears 2 time.\n",
            "Word 870 (\"thing\") appears 1 time.\n",
            "Word 887 (\"tri\") appears 1 time.\n",
            "Word 906 (\"unexpect\") appears 1 time.\n",
            "Word 930 (\"want\") appears 3 time.\n",
            "Word 937 (\"wear\") appears 1 time.\n",
            "Word 948 (\"window\") appears 1 time.\n",
            "Word 953 (\"wise\") appears 1 time.\n",
            "Word 1006 (\"fall\") appears 1 time.\n",
            "Word 1031 (\"person\") appears 3 time.\n",
            "Word 1050 (\"sign\") appears 2 time.\n",
            "Word 1067 (\"year\") appears 3 time.\n",
            "Word 1094 (\"think\") appears 2 time.\n",
            "Word 1122 (\"imagin\") appears 1 time.\n",
            "Word 1135 (\"onlin\") appears 1 time.\n",
            "Word 1165 (\"commit\") appears 3 time.\n",
            "Word 1175 (\"girlfriend\") appears 1 time.\n",
            "Word 1177 (\"guess\") appears 1 time.\n",
            "Word 1220 (\"paper\") appears 1 time.\n",
            "Word 1241 (\"marri\") appears 3 time.\n",
            "Word 1283 (\"financi\") appears 1 time.\n",
            "Word 1295 (\"perfect\") appears 1 time.\n",
            "Word 1309 (\"watch\") appears 1 time.\n",
            "Word 1319 (\"blog\") appears 2 time.\n",
            "Word 1321 (\"bright\") appears 1 time.\n",
            "Word 1326 (\"deal\") appears 2 time.\n",
            "Word 1336 (\"keep\") appears 1 time.\n",
            "Word 1344 (\"outsid\") appears 1 time.\n",
            "Word 1352 (\"singl\") appears 3 time.\n",
            "Word 1356 (\"wall\") appears 1 time.\n",
            "Word 1379 (\"letter\") appears 1 time.\n",
            "Word 1380 (\"love\") appears 1 time.\n",
            "Word 1399 (\"buy\") appears 3 time.\n",
            "Word 1499 (\"nice\") appears 1 time.\n",
            "Word 1651 (\"replac\") appears 1 time.\n",
            "Word 1667 (\"bunch\") appears 1 time.\n",
            "Word 1734 (\"away\") appears 1 time.\n",
            "Word 1757 (\"summer\") appears 1 time.\n",
            "Word 1766 (\"concern\") appears 1 time.\n",
            "Word 2091 (\"felt\") appears 1 time.\n",
            "Word 2102 (\"scari\") appears 1 time.\n",
            "Word 2122 (\"ball\") appears 1 time.\n",
            "Word 2229 (\"search\") appears 1 time.\n",
            "Word 2286 (\"catch\") appears 1 time.\n",
            "Word 2299 (\"mind\") appears 1 time.\n",
            "Word 2359 (\"relationship\") appears 2 time.\n",
            "Word 2460 (\"grow\") appears 1 time.\n",
            "Word 2481 (\"smile\") appears 1 time.\n",
            "Word 2497 (\"stag\") appears 1 time.\n",
            "Word 2694 (\"bare\") appears 1 time.\n",
            "Word 2849 (\"claim\") appears 1 time.\n",
            "Word 2878 (\"wash\") appears 1 time.\n",
            "Word 2898 (\"mental\") appears 1 time.\n",
            "Word 3026 (\"jungl\") appears 1 time.\n",
            "Word 3105 (\"ring\") appears 1 time.\n",
            "Word 3394 (\"paint\") appears 1 time.\n",
            "Word 3969 (\"monday\") appears 1 time.\n",
            "Word 4552 (\"win\") appears 1 time.\n",
            "Word 4662 (\"picki\") appears 1 time.\n",
            "Word 7332 (\"richmond\") appears 1 time.\n",
            "Word 7472 (\"phobia\") appears 1 time.\n",
            "Word 7554 (\"condo\") appears 4 time.\n",
            "Word 7586 (\"deduct\") appears 1 time.\n",
            "Word 7587 (\"mow\") appears 1 time.\n",
            "Word 7588 (\"poof\") appears 1 time.\n",
            "Word 7589 (\"prune\") appears 1 time.\n",
            "Word 7590 (\"realtor\") appears 3 time.\n",
            "Word 7591 (\"roof\") appears 1 time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij941AsfEfbF"
      },
      "source": [
        "### **TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okRakP44DQl7"
      },
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxOG1mnMEx6E",
        "outputId": "2455aef9-65ff-4323-b766-ec28b9e52eac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "corpus_tfidf[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0.3298123436868355),\n",
              " (1, 0.3558990332081455),\n",
              " (2, 0.3585221676972244),\n",
              " (3, 0.3338008779074768),\n",
              " (4, 0.21615377189705884),\n",
              " (5, 0.25751837545870365),\n",
              " (6, 0.29673467081264626),\n",
              " (7, 0.27147372926271435),\n",
              " (8, 0.46649600154635723),\n",
              " (9, 0.17942237454157067)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaxrFjxoFLOU"
      },
      "source": [
        "## **Running LDA using Bag of Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5AX2KQ1FCFa"
      },
      "source": [
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=22, id2word=dictionary, passes=1, workers=3)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQjK_WDGLQc2",
        "outputId": "83fd0f76-ca77-47b3-8878-e0f9f9f5cd0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.032*\"go\" + 0.025*\"like\" + 0.016*\"good\" + 0.014*\"today\" + 0.012*\"yeah\" + 0.010*\"come\" + 0.009*\"think\" + 0.009*\"watch\" + 0.009*\"gonna\" + 0.009*\"play\"\n",
            "Topic: 1 \n",
            "Words: 0.026*\"say\" + 0.017*\"tell\" + 0.017*\"year\" + 0.015*\"famili\" + 0.014*\"go\" + 0.013*\"know\" + 0.013*\"time\" + 0.013*\"like\" + 0.012*\"think\" + 0.011*\"want\"\n",
            "Topic: 2 \n",
            "Words: 0.015*\"like\" + 0.012*\"look\" + 0.009*\"wear\" + 0.009*\"hair\" + 0.006*\"black\" + 0.006*\"room\" + 0.006*\"hous\" + 0.006*\"white\" + 0.006*\"water\" + 0.005*\"walk\"\n",
            "Topic: 3 \n",
            "Words: 0.786*\"nbsp\" + 0.004*\"think\" + 0.004*\"like\" + 0.003*\"time\" + 0.003*\"know\" + 0.003*\"go\" + 0.002*\"want\" + 0.002*\"good\" + 0.002*\"come\" + 0.002*\"look\"\n",
            "Topic: 4 \n",
            "Words: 0.032*\"peopl\" + 0.023*\"think\" + 0.016*\"like\" + 0.016*\"thing\" + 0.013*\"work\" + 0.011*\"go\" + 0.009*\"fuck\" + 0.009*\"money\" + 0.007*\"know\" + 0.007*\"problem\"\n",
            "Topic: 5 \n",
            "Words: 0.224*\"urllink\" + 0.029*\"movi\" + 0.013*\"film\" + 0.009*\"pictur\" + 0.006*\"watch\" + 0.006*\"charact\" + 0.006*\"bring\" + 0.005*\"http\" + 0.005*\"photo\" + 0.005*\"link\"\n",
            "Topic: 6 \n",
            "Words: 0.010*\"like\" + 0.008*\"think\" + 0.006*\"write\" + 0.006*\"band\" + 0.006*\"time\" + 0.006*\"record\" + 0.006*\"music\" + 0.005*\"vote\" + 0.005*\"peopl\" + 0.005*\"michael\"\n",
            "Topic: 7 \n",
            "Words: 0.017*\"life\" + 0.015*\"love\" + 0.009*\"know\" + 0.009*\"time\" + 0.008*\"world\" + 0.008*\"live\" + 0.007*\"thing\" + 0.007*\"feel\" + 0.006*\"think\" + 0.006*\"heart\"\n",
            "Topic: 8 \n",
            "Words: 0.040*\"game\" + 0.025*\"play\" + 0.017*\"team\" + 0.009*\"year\" + 0.007*\"player\" + 0.007*\"time\" + 0.006*\"sport\" + 0.006*\"season\" + 0.005*\"race\" + 0.005*\"lose\"\n",
            "Topic: 9 \n",
            "Words: 0.037*\"know\" + 0.032*\"like\" + 0.029*\"think\" + 0.024*\"want\" + 0.019*\"thing\" + 0.019*\"feel\" + 0.015*\"time\" + 0.015*\"friend\" + 0.014*\"peopl\" + 0.012*\"love\"\n",
            "Topic: 10 \n",
            "Words: 0.042*\"love\" + 0.016*\"lang\" + 0.009*\"level\" + 0.009*\"naman\" + 0.009*\"hindi\" + 0.008*\"para\" + 0.008*\"yung\" + 0.008*\"kung\" + 0.007*\"pero\" + 0.006*\"yang\"\n",
            "Topic: 11 \n",
            "Words: 0.018*\"drink\" + 0.016*\"food\" + 0.013*\"like\" + 0.010*\"eat\" + 0.008*\"chicken\" + 0.007*\"smoke\" + 0.007*\"water\" + 0.006*\"cook\" + 0.006*\"tast\" + 0.006*\"good\"\n",
            "Topic: 12 \n",
            "Words: 0.028*\"haha\" + 0.019*\"go\" + 0.016*\"like\" + 0.013*\"today\" + 0.010*\"hehe\" + 0.010*\"time\" + 0.009*\"wanna\" + 0.008*\"home\" + 0.007*\"dunno\" + 0.007*\"come\"\n",
            "Topic: 13 \n",
            "Words: 0.012*\"look\" + 0.010*\"hand\" + 0.009*\"say\" + 0.009*\"head\" + 0.008*\"come\" + 0.007*\"walk\" + 0.007*\"eye\" + 0.007*\"face\" + 0.006*\"turn\" + 0.006*\"know\"\n",
            "Topic: 14 \n",
            "Words: 0.027*\"quotejil\" + 0.020*\"quotejoel\" + 0.013*\"robot\" + 0.010*\"matrix\" + 0.008*\"linux\" + 0.008*\"vlad\" + 0.008*\"luke\" + 0.007*\"smith\" + 0.006*\"whore\" + 0.005*\"virus\"\n",
            "Topic: 15 \n",
            "Words: 0.049*\"song\" + 0.034*\"music\" + 0.018*\"listen\" + 0.017*\"sing\" + 0.014*\"band\" + 0.014*\"like\" + 0.013*\"love\" + 0.013*\"play\" + 0.009*\"rock\" + 0.009*\"album\"\n",
            "Topic: 16 \n",
            "Words: 0.034*\"blog\" + 0.027*\"post\" + 0.021*\"read\" + 0.014*\"book\" + 0.013*\"write\" + 0.011*\"site\" + 0.010*\"urllink\" + 0.009*\"page\" + 0.009*\"link\" + 0.009*\"comment\"\n",
            "Topic: 17 \n",
            "Words: 0.025*\"school\" + 0.021*\"class\" + 0.013*\"year\" + 0.011*\"work\" + 0.010*\"student\" + 0.009*\"test\" + 0.009*\"studi\" + 0.009*\"time\" + 0.008*\"teacher\" + 0.008*\"learn\"\n",
            "Topic: 18 \n",
            "Words: 0.009*\"bush\" + 0.009*\"say\" + 0.008*\"state\" + 0.007*\"american\" + 0.007*\"peopl\" + 0.006*\"presid\" + 0.006*\"countri\" + 0.006*\"nation\" + 0.006*\"govern\" + 0.006*\"iraq\"\n",
            "Topic: 19 \n",
            "Words: 0.009*\"citi\" + 0.009*\"peopl\" + 0.008*\"friend\" + 0.007*\"place\" + 0.007*\"like\" + 0.006*\"girl\" + 0.006*\"drink\" + 0.006*\"know\" + 0.006*\"go\" + 0.005*\"danc\"\n",
            "Topic: 20 \n",
            "Words: 0.033*\"go\" + 0.023*\"work\" + 0.020*\"time\" + 0.015*\"week\" + 0.014*\"night\" + 0.013*\"home\" + 0.012*\"today\" + 0.012*\"hour\" + 0.011*\"good\" + 0.010*\"come\"\n",
            "Topic: 21 \n",
            "Words: 0.008*\"compani\" + 0.008*\"time\" + 0.008*\"phone\" + 0.007*\"store\" + 0.007*\"need\" + 0.007*\"sell\" + 0.007*\"price\" + 0.006*\"weight\" + 0.006*\"money\" + 0.006*\"card\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gRirjNo5w0Z",
        "outputId": "9b580a98-d77e-4d30-8527-95c3deec0d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Compute Coherence Score using c_v\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=train['clean_doc'], dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score:  0.47214424226038093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ByIop6AOS3n"
      },
      "source": [
        "with open('/content/drive/My Drive/lda_model.pickle', 'wb') as handle:\n",
        "    pickle.dump(lda_model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5bJjmkpOWF5"
      },
      "source": [
        "with open('/content/drive/My Drive/lda_model.pickle', 'rb') as handle:\n",
        "    lda_model = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idYv1GCMFVv-",
        "outputId": "4829e5eb-2657-4dd0-bbf7-cf2cb973769c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
        "\n",
        "# preprocessing and to dictionary\n",
        "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
        "\n",
        "# topic scores\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.6309009194374084\t Topic: 0.009*\"bush\" + 0.009*\"say\" + 0.008*\"state\" + 0.007*\"american\" + 0.007*\"peopl\"\n",
            "Score: 0.21758389472961426\t Topic: 0.034*\"blog\" + 0.027*\"post\" + 0.021*\"read\" + 0.014*\"book\" + 0.013*\"write\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RjkjMyd89jd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}