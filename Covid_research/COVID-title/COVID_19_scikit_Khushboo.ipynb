{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVID-19-scikit-Khushboo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIIp-CUWh-wx",
        "outputId": "5671c419-74e4-42b1-9b5d-3984bcc9dd00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "stop = set(stopwords.words('english'))\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim import corpora, models\n",
        "import gensim\n",
        "import string\n",
        "exclude = set(string.punctuation)\n",
        "lemma = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WMISOyXiJn5"
      },
      "source": [
        "import csv\n",
        "from csv import reader\n",
        "tit = []\n",
        "with open(\"/content/COVID-19_title.csv\", 'r') as read_obj:\n",
        "    # pass the file object to reader() to get the reader object\n",
        "    csv_reader = reader(read_obj)\n",
        "    # Iterate over each row in the csv using reader object\n",
        "    for row in csv_reader:\n",
        "        # row variable is a list that represents a row in csv\n",
        "        tit.append(row)\n",
        "\n",
        "val = []\n",
        "for i in tit:\n",
        "  val.append(i[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1RHaZLfjDa2",
        "outputId": "d07b8e06-5973-4ef1-f02c-394b575f3718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(len(val))\n",
        "print(val[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "242108\n",
            "['clinical features culture-proven mycoplasma pneumoniae infections king abdulaziz university hospital jeddah saudi arabia', 'nitric oxide pro-inflammatory mediator lung diseas', 'surfactant protein-d pulmonary host defens', 'role endothelin-1 lung diseas', 'gene expression epithelial cells response pneumovirus infect', 'sequence requirements strand transfer nidovirus discontinuous subgenomic synthesi', 'debate transfusing normal haemoglobin levels improve outcom', '21st international symposium intensive care emergency medicine brussels belgium 20-23 march 2001', 'heme oxygenase-1 carbon monoxide pulmonary medicin', 'technical description rods real-time public health surveillance system']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQzSX3khjzwD"
      },
      "source": [
        "Document Word Matrix has been created. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU8IZ7MAjFqf"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer='word',       \n",
        "                             min_df=10,                        # minimum reqd occurences of a word \n",
        "                             stop_words='english',             # remove stop words\n",
        "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
        "                             \n",
        "                            )\n",
        "\n",
        "data_vectorized = vectorizer.fit_transform(val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cRjN7Wvj8nr"
      },
      "source": [
        "Running LDA using scikit-learn library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_9cOTQ9kAMA",
        "outputId": "fe168ba5-c612-4ad0-c60b-6eb88faa0d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "\n",
        "# Build LDA Model\n",
        "lda_model = LatentDirichletAllocation(n_components=8,               # Number of topics\n",
        "                                      max_iter=10,               # Max learning iterations\n",
        "                                      learning_method='online',   \n",
        "                                      random_state=42,          # Random state\n",
        "                                      batch_size=128,            # n docs in each learning iter\n",
        "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
        "                                      n_jobs = -1,               # Use all available CPUs\n",
        "                                     )\n",
        "lda_output = lda_model.fit_transform(data_vectorized)\n",
        "\n",
        "print(lda_model)  # Model attributes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
            "                          evaluate_every=-1, learning_decay=0.7,\n",
            "                          learning_method='online', learning_offset=10.0,\n",
            "                          max_doc_update_iter=100, max_iter=10,\n",
            "                          mean_change_tol=0.001, n_components=8, n_jobs=-1,\n",
            "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
            "                          total_samples=1000000.0, verbose=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUxEavyGkTY-",
        "outputId": "53108b89-26a4-4cb0-eccf-f5ff88a357f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Log Likelyhood: Higher the better\n",
        "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
        "\n",
        "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
        "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
        "\n",
        "# See model parameters\n",
        "pprint(lda_model.get_params())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Log Likelihood:  -16092887.728436885\n",
            "Perplexity:  3071.7245349910804\n",
            "{'batch_size': 128,\n",
            " 'doc_topic_prior': None,\n",
            " 'evaluate_every': -1,\n",
            " 'learning_decay': 0.7,\n",
            " 'learning_method': 'online',\n",
            " 'learning_offset': 10.0,\n",
            " 'max_doc_update_iter': 100,\n",
            " 'max_iter': 10,\n",
            " 'mean_change_tol': 0.001,\n",
            " 'n_components': 8,\n",
            " 'n_jobs': -1,\n",
            " 'perp_tol': 0.1,\n",
            " 'random_state': 42,\n",
            " 'topic_word_prior': None,\n",
            " 'total_samples': 1000000.0,\n",
            " 'verbose': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrI2Cw6YoyL2"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define Search Param\n",
        "search_params = {'n_components': [7, 8, 9, 10, 11, 12], 'learning_decay': [.5, .7, .9]}\n",
        "\n",
        "# Init the Model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda, param_grid=search_params)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(data_vectorized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0hCMXFyk5kv"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(model.cv_results_)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-WD48qmgagR"
      },
      "source": [
        "# Best Model\n",
        "best_lda_model = model.best_estimator_\n",
        "\n",
        "# Model Parameters\n",
        "print(\"Best Model's Params: \", model.best_params_)\n",
        "\n",
        "# Log Likelihood Score\n",
        "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
        "\n",
        "# Perplexity\n",
        "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCtvlYxGginp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}