{"cells":[{"metadata":{},"cell_type":"markdown","source":["<h1 align=\"center\">Topic modeling on Medium articles</h1>"]},{"source":["## Minimum setup and libraries required"],"cell_type":"markdown","metadata":{}},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["## All minimum setup and libraries required\n","\n","import numpy as np\n","import pandas as pd \n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","## TENSORFLOW        \n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer   ## Generate dictionary of word encodings\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","## GENSIM and NLTK\n","import gensim\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","import nltk\n","nltk.download('wordnet')\n","\n","# SKLEARN\n","from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import GridSearchCV\n","from pprint import pprint\n","\n","print(\"Tensorflow\\t-\\t\",tf.__version__)\n","print(\"NLTK\\t\\t-\\t\",nltk.__version__)\n","print(\"Gensim\\t\\t-\\t\",nltk.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/medium-articles-with-content/Medium_AggregatedData.csv\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nTensorflow\t-\t 2.3.0\nNLTK\t\t-\t 3.2.4\nGensim\t\t-\t 3.2.4\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":["path = \"../input/medium-articles-with-content/Medium_AggregatedData.csv\"\n","dataframe_full = pd.read_csv(path)\n","dataframe_imp = pd.read_csv(path)\n","print(\"Dataset have been read\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":"Dataset have been read\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":["dataframe_full.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   audioVersionDurationSec codeBlock  codeBlockCount  collectionId  \\\n0                        0       NaN             0.0  638f418c8464   \n1                        0       NaN             0.0  638f418c8464   \n2                        0       NaN             0.0  638f418c8464   \n3                        0       NaN             0.0           NaN   \n4                        0       NaN             0.0           NaN   \n\n  createdDate      createdDatetime firstPublishedDate firstPublishedDatetime  \\\n0  2018-09-18  2018-09-18 20:55:34         2018-09-18    2018-09-18 20:57:03   \n1  2018-09-18  2018-09-18 20:55:34         2018-09-18    2018-09-18 20:57:03   \n2  2018-09-18  2018-09-18 20:55:34         2018-09-18    2018-09-18 20:57:03   \n3  2018-01-07  2018-01-07 17:04:37         2018-01-07    2018-01-07 17:06:29   \n4  2018-01-07  2018-01-07 17:04:37         2018-01-07    2018-01-07 17:06:29   \n\n   imageCount  isSubscriptionLocked  ...        slug        name postCount  \\\n0           1                 False  ...  blockchain  Blockchain  265164.0   \n1           1                 False  ...     samsung     Samsung    5708.0   \n2           1                 False  ...          it          It    3720.0   \n3          13                 False  ...  technology  Technology  166125.0   \n4          13                 False  ...    robotics    Robotics    9103.0   \n\n         author  bio        userId    userName  usersFollowedByCount  \\\n0   Anar Babaev  NaN  f1ad85af0169  babaevanar                 450.0   \n1   Anar Babaev  NaN  f1ad85af0169  babaevanar                 450.0   \n2   Anar Babaev  NaN  f1ad85af0169  babaevanar                 450.0   \n3  George Sykes  NaN  93b9e94f08ca    tasty231                   6.0   \n4  George Sykes  NaN  93b9e94f08ca    tasty231                   6.0   \n\n   usersFollowedCount scrappedDate  \n0               404.0     20181104  \n1               404.0     20181104  \n2               404.0     20181104  \n3                22.0     20181104  \n4                22.0     20181104  \n\n[5 rows x 50 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audioVersionDurationSec</th>\n      <th>codeBlock</th>\n      <th>codeBlockCount</th>\n      <th>collectionId</th>\n      <th>createdDate</th>\n      <th>createdDatetime</th>\n      <th>firstPublishedDate</th>\n      <th>firstPublishedDatetime</th>\n      <th>imageCount</th>\n      <th>isSubscriptionLocked</th>\n      <th>...</th>\n      <th>slug</th>\n      <th>name</th>\n      <th>postCount</th>\n      <th>author</th>\n      <th>bio</th>\n      <th>userId</th>\n      <th>userName</th>\n      <th>usersFollowedByCount</th>\n      <th>usersFollowedCount</th>\n      <th>scrappedDate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>638f418c8464</td>\n      <td>2018-09-18</td>\n      <td>2018-09-18 20:55:34</td>\n      <td>2018-09-18</td>\n      <td>2018-09-18 20:57:03</td>\n      <td>1</td>\n      <td>False</td>\n      <td>...</td>\n      <td>blockchain</td>\n      <td>Blockchain</td>\n      <td>265164.0</td>\n      <td>Anar Babaev</td>\n      <td>NaN</td>\n      <td>f1ad85af0169</td>\n      <td>babaevanar</td>\n      <td>450.0</td>\n      <td>404.0</td>\n      <td>20181104</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>638f418c8464</td>\n      <td>2018-09-18</td>\n      <td>2018-09-18 20:55:34</td>\n      <td>2018-09-18</td>\n      <td>2018-09-18 20:57:03</td>\n      <td>1</td>\n      <td>False</td>\n      <td>...</td>\n      <td>samsung</td>\n      <td>Samsung</td>\n      <td>5708.0</td>\n      <td>Anar Babaev</td>\n      <td>NaN</td>\n      <td>f1ad85af0169</td>\n      <td>babaevanar</td>\n      <td>450.0</td>\n      <td>404.0</td>\n      <td>20181104</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>638f418c8464</td>\n      <td>2018-09-18</td>\n      <td>2018-09-18 20:55:34</td>\n      <td>2018-09-18</td>\n      <td>2018-09-18 20:57:03</td>\n      <td>1</td>\n      <td>False</td>\n      <td>...</td>\n      <td>it</td>\n      <td>It</td>\n      <td>3720.0</td>\n      <td>Anar Babaev</td>\n      <td>NaN</td>\n      <td>f1ad85af0169</td>\n      <td>babaevanar</td>\n      <td>450.0</td>\n      <td>404.0</td>\n      <td>20181104</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>2018-01-07</td>\n      <td>2018-01-07 17:04:37</td>\n      <td>2018-01-07</td>\n      <td>2018-01-07 17:06:29</td>\n      <td>13</td>\n      <td>False</td>\n      <td>...</td>\n      <td>technology</td>\n      <td>Technology</td>\n      <td>166125.0</td>\n      <td>George Sykes</td>\n      <td>NaN</td>\n      <td>93b9e94f08ca</td>\n      <td>tasty231</td>\n      <td>6.0</td>\n      <td>22.0</td>\n      <td>20181104</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>2018-01-07</td>\n      <td>2018-01-07 17:04:37</td>\n      <td>2018-01-07</td>\n      <td>2018-01-07 17:06:29</td>\n      <td>13</td>\n      <td>False</td>\n      <td>...</td>\n      <td>robotics</td>\n      <td>Robotics</td>\n      <td>9103.0</td>\n      <td>George Sykes</td>\n      <td>NaN</td>\n      <td>93b9e94f08ca</td>\n      <td>tasty231</td>\n      <td>6.0</td>\n      <td>22.0</td>\n      <td>20181104</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 50 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["x = dataframe_full['name'][10]\n","y = dataframe_full['publicationdescription'][15]\n","print(x)\n","print(y)\n","print(dataframe_full.shape)\n","print(dataframe_full['name'][10])\n","print(dataframe_full['name'][11])\n","print(dataframe_full['name'][12])\n","print(dataframe_full['title'][10])"],"execution_count":4,"outputs":[{"output_type":"stream","text":"Big Data Training Mumbai\nNon-obvious meditation advice from people on the battlefront of daily creation\n(279577, 50)\nBig Data Training Mumbai\nRobotics\nMeditation\nAscent of data Science, SAS and Big data Analyst Trainings Programs\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["# Step_1: Preprocessing and cleaning"]},{"metadata":{},"cell_type":"markdown","source":["## There are ~300000 entries"]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(dataframe_full.columns)"],"execution_count":5,"outputs":[{"output_type":"stream","text":"Index(['audioVersionDurationSec', 'codeBlock', 'codeBlockCount',\n       'collectionId', 'createdDate', 'createdDatetime', 'firstPublishedDate',\n       'firstPublishedDatetime', 'imageCount', 'isSubscriptionLocked',\n       'language', 'latestPublishedDate', 'latestPublishedDatetime',\n       'linksCount', 'postId', 'readingTime', 'recommends',\n       'responsesCreatedCount', 'socialRecommendsCount', 'subTitle',\n       'tagsCount', 'text', 'title', 'totalClapCount', 'uniqueSlug',\n       'updatedDate', 'updatedDatetime', 'url', 'vote', 'wordCount',\n       'publicationdescription', 'publicationdomain',\n       'publicationfacebookPageName', 'publicationfollowerCount',\n       'publicationname', 'publicationpublicEmail', 'publicationslug',\n       'publicationtags', 'publicationtwitterUsername', 'tag_name', 'slug',\n       'name', 'postCount', 'author', 'bio', 'userId', 'userName',\n       'usersFollowedByCount', 'usersFollowedCount', 'scrappedDate'],\n      dtype='object')\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["### Required columns:\n","\n","* language\n","\n","* subTitle\n","\n","* tagsCount\n","\n","* text\n","\n","* title\n","\n","* url\n","\n","* wordCount\n","\n","* publicationdescription\n","\n","* tag_name\n","\n","* name\n","\n","### Primary columns:\n","\n","* subTitle\n","\n","* text\n","\n","* title"]},{"metadata":{"trusted":true},"cell_type":"code","source":["required_col = ['language','subTitle','tagsCount','text','title','url','wordCount','publicationdescription'\n","               ,'tag_name','name']\n","most_imp_col = ['subTitle','text','title']"],"execution_count":6,"outputs":[]},{"source":["## Removing Null rows"],"cell_type":"markdown","metadata":{}},{"metadata":{"trusted":true},"cell_type":"code","source":["# Number of rows dropped after removing null value rows\n","\n","print(dataframe_imp.shape)\n","dataframe_imp.dropna(how = 'all')\n","print(dataframe_imp.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":"(279577, 50)\n(279577, 50)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["So missing rows, so no null values"]},{"source":["## Available languages"],"cell_type":"markdown","metadata":{}},{"metadata":{"trusted":true},"cell_type":"code","source":["# article_titles = dataframe_full['title']\n","# art_grp_1 = article_titles[16:25]\n","# print(art_grp_1)\n","print(dataframe_full.language.unique())"],"execution_count":7,"outputs":[{"output_type":"stream","text":"['en' 'th' 'ja' 'zh' 'ru' 'pt' 'es' 'zh-Hant' 'id' 'my' 'de' 'tr' 'fr'\n 'ko' 'it' 'lo' 'un' 'vi' 'cs' 'sk' 'is' 'sv' 'bn' 'mn' 'da' 'no' 'bg'\n 'ar' 'pl' 'nl' 'ro' 'ca' 'hu' 'hi' 'ka' 'el' 'ms' 'uk' 'si' 'sr' 'lt'\n 'la' 'fa' 'ml' 'sl' 'mr' 'az' 'lv' 'te' 'mk' 'nn' 'fi']\n","name":"stdout"}]},{"source":["## Number of English rows"],"cell_type":"markdown","metadata":{}},{"metadata":{"trusted":true},"cell_type":"code","source":["english_titles = dataframe_full[dataframe_full['language'] == 'en']\n","# english_titles.head()\n","print(english_titles.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":"(257655, 50)\n","name":"stdout"}]},{"source":["## Removing Non - English Rows and Unnecessary Columns"],"cell_type":"markdown","metadata":{}},{"metadata":{"trusted":true},"cell_type":"code","source":["## After dropping non-english and columns that are not required really\n","\n","dataframe_imp.drop(dataframe_imp[dataframe_imp['language'] != 'en'].index, inplace = True)\n","\n","dataframe_imp = dataframe_imp.drop(['audioVersionDurationSec', 'codeBlock', 'codeBlockCount',\n","       'collectionId', 'createdDate', 'createdDatetime', 'firstPublishedDate',\n","       'firstPublishedDatetime', 'imageCount', 'isSubscriptionLocked',\n","       'language', 'latestPublishedDate', 'latestPublishedDatetime',\n","       'linksCount', 'postId', 'readingTime', 'recommends',\n","       'responsesCreatedCount', 'socialRecommendsCount','tagsCount','totalClapCount', 'uniqueSlug',\n","       'updatedDate', 'updatedDatetime', 'url', 'vote', 'wordCount',\n","       'publicationdescription', 'publicationdomain',\n","       'publicationfacebookPageName', 'publicationfollowerCount',\n","       'publicationname', 'publicationpublicEmail', 'publicationslug',\n","       'publicationtags', 'publicationtwitterUsername', 'tag_name', 'slug',\n","       'name', 'postCount', 'author', 'bio', 'userId', 'userName',\n","       'usersFollowedByCount', 'usersFollowedCount', 'scrappedDate'], axis=1)\n","\n","dataframe_imp['index'] = dataframe_imp.index\n","\n","dataframe_imp.shape"],"execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(257655, 4)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":["## Reduced dataset with 3 columns and ~20000 rows for reference\n","### Run these cells only to produce and export the reduced and compact dataset"]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Run these cell to export the new trimmed down dataset\n","\n","# dataframe_imp.to_csv(\"medium_dataset.csv\",sep=\",\")\n","# new_ds_size = os.stat(\"medium_dataset.csv\").st_size\n","# new_ds_size = new_ds_size / 1000000\n","# print(\"New dataset size in MB = \",new_ds_size)"],"execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["***Choose one among the two in the cell below***"]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Keeping 170000 rows greatly reduces the dataset size to around 100MB\n","\n","very_reduced_dataset = dataframe_imp[:17000]\n","# very_reduced_dataset.to_csv(\"very_reduced_dataset.csv\",sep=\",\")\n","# print(\"New dataset size in MB = \",os.stat(\"very_reduced_dataset.csv\").st_size / 1000000)\n","\n","# from IPython.display import FileLink\n","# FileLink(r'very_reduced_dataset.csv')"],"execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["***After dropping all the non-english rows and all non essential columns `dataframe_imp` is the required dataframe***"]},{"metadata":{"trusted":true},"cell_type":"code","source":["dataframe_imp.head()"],"execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"                                            subTitle  \\\n0  A major private IT company implements blockcha...   \n1  A major private IT company implements blockcha...   \n2  A major private IT company implements blockcha...   \n3                                       Introduction   \n4                                       Introduction   \n\n                                                text  \\\n0  Private Business, Government and Blockchain\\n\\...   \n1  Private Business, Government and Blockchain\\n\\...   \n2  Private Business, Government and Blockchain\\n\\...   \n3  EPQ draft 1 (4844 words)\\nhttps://upload.wikim...   \n4  EPQ draft 1 (4844 words)\\nhttps://upload.wikim...   \n\n                                         title  index  \n0  Private Business, Government and Blockchain      0  \n1  Private Business, Government and Blockchain      1  \n2  Private Business, Government and Blockchain      2  \n3                     EPQ draft 1 (4844 words)      3  \n4                     EPQ draft 1 (4844 words)      4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subTitle</th>\n      <th>text</th>\n      <th>title</th>\n      <th>index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A major private IT company implements blockcha...</td>\n      <td>Private Business, Government and Blockchain\\n\\...</td>\n      <td>Private Business, Government and Blockchain</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A major private IT company implements blockcha...</td>\n      <td>Private Business, Government and Blockchain\\n\\...</td>\n      <td>Private Business, Government and Blockchain</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A major private IT company implements blockcha...</td>\n      <td>Private Business, Government and Blockchain\\n\\...</td>\n      <td>Private Business, Government and Blockchain</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Introduction</td>\n      <td>EPQ draft 1 (4844 words)\\nhttps://upload.wikim...</td>\n      <td>EPQ draft 1 (4844 words)</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Introduction</td>\n      <td>EPQ draft 1 (4844 words)\\nhttps://upload.wikim...</td>\n      <td>EPQ draft 1 (4844 words)</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(dataframe_imp.title[15])\n","print(dataframe_imp.subTitle[15])\n","# print(dataframe_imp.text[15])      ## Text is too huge to be displayed\n","print(dataframe_imp.index[15])"],"execution_count":14,"outputs":[{"output_type":"stream","text":"Can a robot love us better than another human can?\nI discussed this with Michelle Tsng on my Podcast “Crazy Wisdom”.\n15\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["## Perform lemmatization and stem preprocessing steps on the data set\n"]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Stemmer initialization for english\n","stemmer = SnowballStemmer(\"english\")"],"execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Functions for lemmatization, removal of Stopwords\n","\n","def lemmatize_stemming(text):\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n","            result.append(lemmatize_stemming(token))\n","    return result"],"execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["### Code to check the function\n","\n","## Run this for faster execution time wiht less acuracy\n","doc_sample = very_reduced_dataset[very_reduced_dataset['index'] == 1000].values[0][2]\n","\n","## Run this for slower execution but better accuracy\n","# doc_sample = dataframe_imp[dataframe_imp['index'] == 1000].values[0][2]\n","\n","print('original document: ')\n","words = []\n","for word in doc_sample.split(' '):\n","    words.append(word)\n","    \n","print(dataframe_imp[dataframe_imp['index'] == 1000].values[0][2])\n","print(words)\n","print('\\n\\n tokenized and lemmatized document: ')\n","print(preprocess(doc_sample))"],"execution_count":17,"outputs":[{"output_type":"stream","text":"original document: \nMachine Learning Made Easy: What it is and How it Works\n['Machine', 'Learning', 'Made', 'Easy:', 'What', 'it', 'is', 'and', 'How', 'it', 'Works']\n\n\n tokenized and lemmatized document: \n['machin', 'learn', 'easi', 'work']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":["## Processed titles"]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Use this to reduce the training time at the cost of loss of rows and some accuracy.\n","title_list = very_reduced_dataset['title'].astype(str)\n","\n","## Use this for higher model accuracy but very slow operating time. \n","# title_list = dataframe_imp['title'].astype(str)   ## using astype(str) eliminates the floting type error\n","title_list.describe()"],"execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"count                           17000\nunique                           4436\ntop       10 new things to read in AI\nfreq                               24\nName: title, dtype: object"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["## The titles are preprocessed and saved into processd_titles\n","## The map function applies the preprocess method on each of the list entries\n","\n","processed_titles = title_list.map(preprocess)\n","processed_titles[30:40]"],"execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"34    [meta, model, meta, meta, model, deep, learn]\n35    [meta, model, meta, meta, model, deep, learn]\n36               [tip, data, scienc, team, succeed]\n37               [tip, data, scienc, team, succeed]\n38               [tip, data, scienc, team, succeed]\n39                                   [trust, trust]\n40                                   [trust, trust]\n41                                   [trust, trust]\n42                                   [trust, trust]\n43                                   [trust, trust]\nName: title, dtype: object"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}